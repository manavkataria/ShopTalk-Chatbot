{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "import os\n",
    "from dataclasses import dataclass\n",
    "import os\n",
    "from langchain_google_genai import ChatGoogleGenerativeAI\n",
    "from langchain_community.embeddings import HuggingFaceEmbeddings\n",
    "from langchain_text_splitters import CharacterTextSplitter\n",
    "import json\n",
    "from dataclasses import dataclass\n",
    "from langchain_text_splitters import CharacterTextSplitter\n",
    "from langchain_community.embeddings import HuggingFaceEmbeddings\n",
    "from langchain_community.vectorstores import FAISS\n",
    "from langgraph.graph import MessagesState, StateGraph\n",
    "from langchain_core.tools import tool\n",
    "from langchain_core.messages import SystemMessage\n",
    "from langgraph.prebuilt import ToolNode\n",
    "from langgraph.graph import END\n",
    "from langgraph.prebuilt import ToolNode, tools_condition\n",
    "from langgraph.checkpoint.memory import MemorySaver\n",
    "\n",
    "\n",
    "file_path = \"listings/metadata/listings_0.json\"\n",
    "\n",
    "# with open(file_path, \"r\", encoding=\"utf-8\") as f:\n",
    "#     data = [json.loads(line) for line in f]  # Read each line as a separate JSON object\n",
    "\n",
    "# n_data_samples = len(data)\n",
    "# print(n_data_samples)  # Number of JSON objects in the file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "api_key = os.getenv('GEMINI_API_KEY')\n",
    "# client = genai.Client(api_key=api_key)\n",
    "\n",
    "# For information on the available models, see:\n",
    "# https://cloud.google.com/vertex-ai/generative-ai/docs/learn/models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "@dataclass\n",
    "class FileAndMeatadata:\n",
    "    file_string: str\n",
    "    item_id: str\n",
    "    main_image_id: str\n",
    "    other_image_id: str\n",
    "\n",
    "class VectorStore:\n",
    "    def __init__(self, file_path,\n",
    "                 embedding_model_name=\"sentence-transformers/paraphrase-multilingual-mpnet-base-v2\", \n",
    "                 chunk_size=1000, \n",
    "                 chunk_overlap=200):\n",
    "        self._embedding_model_name = embedding_model_name\n",
    "        self._file_path = file_path\n",
    "        self._chunk_size = chunk_size\n",
    "        self._chunk_overlap = chunk_overlap\n",
    "        self._embedding_model = HuggingFaceEmbeddings(model_name=embedding_model_name)\n",
    "        # self._tokenizer = AutoTokenizer.from_pretrained(embedding_model_name)\n",
    "        self._file_and_metadata_list = self._load_and_process_json()\n",
    "        self.vectorstore = self._chunk_and_embed()\n",
    "\n",
    "    def perform_search(self, query: str, top_k=1):\n",
    "        \"\"\"Performs a similarity search and returns results.\"\"\"\n",
    "        # TODO: pre-process query if it is too long!\n",
    "        # For this you will need to use self._tokenizer\n",
    "        docs = self.vectorstore.similarity_search(query, k=top_k)\n",
    "        return docs\n",
    "\n",
    "    def _load_and_process_json(self) -> list[FileAndMeatadata]:\n",
    "        \"\"\"Loads JSON data, processes it, and returns a list of FileAndMeatadata objects.\"\"\"\n",
    "        with open(file_path, 'r', encoding='utf-8') as f:\n",
    "            return [self._json_to_str(json.loads(line)) for line in f] \n",
    "        \n",
    "    def _json_to_str(self, file: dict) -> FileAndMeatadata:\n",
    "        item_id = file.pop('item_id', None)\n",
    "        main_image_id = file.pop('main_image_id', None)\n",
    "        other_image_id = file.pop('other_image_id', None)\n",
    "\n",
    "        file.pop('model_number', None)\n",
    "        file.pop('marketplace', None)\n",
    "        file.pop(\"domain_name\", None)\n",
    "\n",
    "        remaining_data_str = self._flatten_json(file)\n",
    "        file_and_metadata = FileAndMeatadata(file_string=remaining_data_str, \n",
    "                                             item_id=item_id, \n",
    "                                             main_image_id=main_image_id, \n",
    "                                             other_image_id=other_image_id)\n",
    "        return file_and_metadata\n",
    "    \n",
    "    def _flatten_json(self, y):\n",
    "        \"\"\"Flatten nested JSON into a plain text format.\"\"\"\n",
    "        out = []\n",
    "\n",
    "        def flatten(x):\n",
    "            if isinstance(x, dict):\n",
    "                for v in x.values():\n",
    "                    flatten(v)\n",
    "            elif isinstance(x, list):\n",
    "                for item in x:\n",
    "                    flatten(item)\n",
    "            elif isinstance(x, str):\n",
    "                out.append(x.lower())\n",
    "\n",
    "        flatten(y)\n",
    "        return \" \".join(out)\n",
    "\n",
    "    def _chunk_and_embed(self) -> FAISS:\n",
    "        \"\"\"Chunks file_string, embeds, and creates a FAISS vector store with metadata.\"\"\"\n",
    "        text_splitter = CharacterTextSplitter(chunk_size=self._chunk_size, chunk_overlap=self._chunk_overlap)\n",
    "        chunks_with_metadata = []\n",
    "\n",
    "        # TODO: smarter chunking based on the number of tokens insted of the number of characters.\n",
    "        for item_idx, item in enumerate(self._file_and_metadata_list):\n",
    "            if len(item.file_string) > self._chunk_size:\n",
    "                text_chunks = text_splitter.split_text(item.file_string)\n",
    "                for chunk in text_chunks:\n",
    "                    chunks_with_metadata.append((chunk, {\n",
    "                        \"item_idx\": item_idx,\n",
    "                        \"item_id\": item.item_id,\n",
    "                        \"main_image_id\": item.main_image_id,\n",
    "                        \"other_image_id\": item.other_image_id\n",
    "                    }))\n",
    "            else:\n",
    "                chunks_with_metadata.append((item.file_string, {\n",
    "                    \"item_idx\": item_idx,\n",
    "                        \"item_id\": item.item_id,\n",
    "                        \"main_image_id\": item.main_image_id,\n",
    "                        \"other_image_id\": item.other_image_id\n",
    "                }))\n",
    "\n",
    "        texts = [chunk[0] for chunk in chunks_with_metadata]\n",
    "        metadatas = [chunk[1] for chunk in chunks_with_metadata]\n",
    "\n",
    "        vectorstore = FAISS.from_texts(texts, self._embedding_model, metadatas=metadatas)\n",
    "        return vectorstore"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Example usage:\n",
    "vector_store = VectorStore(file_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "query = \"iphone cover\"\n",
    "results = vector_store.perform_search(query)\n",
    "results[0].page_content"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "results[0].metadata"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# class RAGShoppingAssistant:\n",
    "#     def __init__(self, vectorstore):\n",
    "#         self.vectorstore = vectorstore\n",
    "#         self._llm = ChatGoogleGenerativeAI(model=\"gemini-1.5-pro\", api_key=os.getenv('GEMINI_API_KEY'))\n",
    "#         self._memory = ConversationSummaryBufferMemory.from_llm(llm=self._llm, memory_key=\"chat_history\", return_messages=True)\n",
    "\n",
    "#         self._rag_chain = self._setup_rag_chain()\n",
    "\n",
    "#     def _setup_rag_chain(self):\n",
    "#         \"\"\"Sets up the RAG chain with conversation memory.\"\"\"\n",
    "#         template = \"\"\"Use the following pieces of context to answer the question at the end.\n",
    "#         If you don't know the answer, just say you don't know, don't try to make up an answer.\n",
    "#         Also, keep the conversation going, and remember the previous questions and answers.\n",
    "#         {context}\n",
    "#         Question: {question}\n",
    "#         {chat_history}\n",
    "#         Answer:\"\"\"\n",
    "#         prompt = ChatPromptTemplate.from_template(template)\n",
    "#         retriever = self.vectorstore.as_retriever()\n",
    "#         rag_chain = ConversationalRetrievalChain.from_llm(\n",
    "#             self._llm,\n",
    "#             retriever,\n",
    "#             memory=self._memory,\n",
    "#             return_source_documents=True,\n",
    "#         )\n",
    "#         return rag_chain\n",
    "\n",
    "#     def chat_with_assistant(self, question):\n",
    "#         result = self._rag_chain({\"question\": question})\n",
    "#         print(f\"Answer: {result['answer']}\")\n",
    "#         for doc in result['source_documents']:\n",
    "#             print(f\"  - {doc.page_content} (Metadata: {doc.metadata})\")\n",
    "#         return result['question']\n",
    "\n",
    "\n",
    "# assistant = RAGShoppingAssistant(vector_store)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Check out this for RAG\n",
    "# Just review this one https://python.langchain.com/docs/tutorials/rag/\n",
    "# Use this one https://python.langchain.com/docs/tutorials/qa_chat_history/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "llm = ChatGoogleGenerativeAI(model=\"gemini-2.0-flash-001\", api_key=os.getenv('GEMINI_API_KEY'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "graph_builder = StateGraph(MessagesState)\n",
    "\n",
    "@tool(response_format=\"content_and_artifact\")\n",
    "def retrieve(query: str):\n",
    "    \"\"\"Retrieve information related to a query.\"\"\"\n",
    "    retrieved_docs = vector_store.perform_search(query, top_k=2)\n",
    "    serialized = \"\\n\\n\".join(\n",
    "        (f\"Content: {doc.page_content}\")\n",
    "        for doc in retrieved_docs\n",
    "    )\n",
    "    return serialized, retrieved_docs\n",
    "\n",
    "# Step 1: Generate an AIMessage that may include a tool-call to be sent.\n",
    "def query_or_respond(state: MessagesState):\n",
    "    \"\"\"Generate tool call for retrieval or respond.\"\"\"\n",
    "    llm_with_tools = llm.bind_tools([retrieve])\n",
    "    response = llm_with_tools.invoke(state[\"messages\"])\n",
    "    # MessagesState appends messages to state instead of overwriting\n",
    "    return {\"messages\": [response]}\n",
    "\n",
    "\n",
    "# Step 2: Execute the retrieval.\n",
    "tools = ToolNode([retrieve])\n",
    "\n",
    "\n",
    "# Step 3: Generate a response using the retrieved content.\n",
    "def generate(state: MessagesState):\n",
    "    \"\"\"Generate answer.\"\"\"\n",
    "    # Get generated ToolMessages\n",
    "    recent_tool_messages = []\n",
    "    for message in reversed(state[\"messages\"]):\n",
    "        if message.type == \"tool\":\n",
    "            recent_tool_messages.append(message)\n",
    "        else:\n",
    "            break\n",
    "    tool_messages = recent_tool_messages[::-1]\n",
    "\n",
    "    # Format into prompt\n",
    "    docs_content = \"\\n\\n\".join(doc.content for doc in tool_messages)\n",
    "    system_message_content = (\n",
    "        \"You are a shopping assistant for question-answering tasks. \"\n",
    "        \"Use the following pieces of retrieved context to answer \"\n",
    "        \"the question. If you don't know the answer, say that you \"\n",
    "        \"don't know. Use three sentences maximum and keep the \"\n",
    "        \"answer concise.\"\n",
    "        \"\\n\\n\"\n",
    "        f\"{docs_content}\"\n",
    "    )\n",
    "    conversation_messages = [\n",
    "        message\n",
    "        for message in state[\"messages\"]\n",
    "        if message.type in (\"human\", \"system\")\n",
    "        or (message.type == \"ai\" and not message.tool_calls)\n",
    "    ]\n",
    "    prompt = [SystemMessage(system_message_content)] + conversation_messages\n",
    "\n",
    "    # Run\n",
    "    response = llm.invoke(prompt)\n",
    "    return {\"messages\": [response]}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "graph_builder.add_node(query_or_respond)\n",
    "graph_builder.add_node(tools)\n",
    "graph_builder.add_node(generate)\n",
    "\n",
    "graph_builder.set_entry_point(\"query_or_respond\")\n",
    "graph_builder.add_conditional_edges(\n",
    "    \"query_or_respond\",\n",
    "    tools_condition,\n",
    "    {END: END, \"tools\": \"tools\"},\n",
    ")\n",
    "graph_builder.add_edge(\"tools\", \"generate\")\n",
    "graph_builder.add_edge(\"generate\", END)\n",
    "\n",
    "memory = MemorySaver()\n",
    "graph = graph_builder.compile(checkpointer=memory)\n",
    "\n",
    "config = {\"configurable\": {\"thread_id\": \"abc123\"}}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# from IPython.display import Image, display\n",
    "\n",
    "# display(Image(graph.get_graph().draw_mermaid_png()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "input_message = \"Hello\"\n",
    "\n",
    "for step in graph.stream(\n",
    "    {\"messages\": [{\"role\": \"user\", \"content\": input_message}]},\n",
    "    stream_mode=\"values\",\n",
    "    config=config\n",
    "):\n",
    "    step[\"messages\"][-1].pretty_print()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "input_message = \"Can you help me to find a phone cover for an iphone?\"\n",
    "\n",
    "for step in graph.stream(\n",
    "    {\"messages\": [{\"role\": \"user\", \"content\": input_message}]},\n",
    "    stream_mode=\"values\",\n",
    "    config=config\n",
    "):\n",
    "    step[\"messages\"][-1].pretty_print()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "input_message = \"Is it available in red?\"\n",
    "\n",
    "for step in graph.stream(\n",
    "    {\"messages\": [{\"role\": \"user\", \"content\": input_message}]},\n",
    "    stream_mode=\"values\",\n",
    "    config=config\n",
    "):\n",
    "    step[\"messages\"][-1].pretty_print()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO: return not only text, but also corresponding images."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# RAG with text & image embedding.\n",
    "# TODO: just use an embedding model accepting both text and image inputs."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "shoptalk-chatbot-_PQObPGz-py3.11",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
