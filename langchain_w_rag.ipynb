{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "import os\n",
    "# import chromadb\n",
    "from sentence_transformers import SentenceTransformer\n",
    "import numpy as np\n",
    "from dataclasses import dataclass\n",
    "import os\n",
    "from google import genai\n",
    "from langchain_google_genai import ChatGoogleGenerativeAI\n",
    "from langchain.chat_models import init_chat_model\n",
    "import langchain\n",
    "from langchain_community.document_loaders import TextLoader\n",
    "# from langchain_google_genai import GoogleGenerativeAIEmbeddings\n",
    "from langchain_community.embeddings import HuggingFaceEmbeddings\n",
    "from langchain_text_splitters import CharacterTextSplitter\n",
    "# from langchain_chroma import Chroma\n",
    "# from langchain_community.vectorstores import FAISS\n",
    "import json\n",
    "from dataclasses import dataclass\n",
    "from langchain_text_splitters import CharacterTextSplitter\n",
    "from langchain_community.embeddings import HuggingFaceEmbeddings\n",
    "from langchain_community.vectorstores import FAISS\n",
    "# from transformers import AutoTokenizer\n",
    "from langchain.prompts import ChatPromptTemplate\n",
    "from langchain.memory import ConversationBufferMemory\n",
    "from langchain.chains import ConversationalRetrievalChain\n",
    "from langchain.memory import ConversationSummaryBufferMemory\n",
    "\n",
    "\n",
    "file_path = \"listings/metadata/listings_0.json\"\n",
    "\n",
    "# with open(file_path, \"r\", encoding=\"utf-8\") as f:\n",
    "#     data = [json.loads(line) for line in f]  # Read each line as a separate JSON object\n",
    "\n",
    "# n_data_samples = len(data)\n",
    "# print(n_data_samples)  # Number of JSON objects in the file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "api_key = os.getenv('GEMINI_API_KEY')\n",
    "# client = genai.Client(api_key=api_key)\n",
    "\n",
    "# For information on the available models, see:\n",
    "# https://cloud.google.com/vertex-ai/generative-ai/docs/learn/models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "@dataclass\n",
    "class FileAndMeatadata:\n",
    "    file_string: str\n",
    "    item_id: str\n",
    "    main_image_id: str\n",
    "    other_image_id: str\n",
    "\n",
    "class VectorStore:\n",
    "    def __init__(self, file_path,\n",
    "                 embedding_model_name=\"sentence-transformers/paraphrase-multilingual-mpnet-base-v2\", \n",
    "                 chunk_size=1000, \n",
    "                 chunk_overlap=200):\n",
    "        self._embedding_model_name = embedding_model_name\n",
    "        self._file_path = file_path\n",
    "        self._chunk_size = chunk_size\n",
    "        self._chunk_overlap = chunk_overlap\n",
    "        self._embedding_model = HuggingFaceEmbeddings(model_name=embedding_model_name)\n",
    "        # self._tokenizer = AutoTokenizer.from_pretrained(embedding_model_name)\n",
    "        self._file_and_metadata_list = self._load_and_process_json()\n",
    "        self.vectorstore = self._chunk_and_embed()\n",
    "\n",
    "    def perform_search(self, query: str, top_k=1):\n",
    "        \"\"\"Performs a similarity search and returns results.\"\"\"\n",
    "        # TODO: pre-process query if it is too long!\n",
    "        # For this you will need to use self._tokenizer\n",
    "        docs = self.vectorstore.similarity_search(query, k=top_k)\n",
    "        return docs\n",
    "\n",
    "    def _load_and_process_json(self) -> list[FileAndMeatadata]:\n",
    "        \"\"\"Loads JSON data, processes it, and returns a list of FileAndMeatadata objects.\"\"\"\n",
    "        with open(file_path, 'r', encoding='utf-8') as f:\n",
    "            return [self._json_to_str(json.loads(line)) for line in f] \n",
    "        \n",
    "    def _json_to_str(self, file: dict) -> FileAndMeatadata:\n",
    "        item_id = file.pop('item_id', None)\n",
    "        main_image_id = file.pop('main_image_id', None)\n",
    "        other_image_id = file.pop('other_image_id', None)\n",
    "\n",
    "        file.pop('model_number', None)\n",
    "        file.pop('marketplace', None)\n",
    "        file.pop(\"domain_name\", None)\n",
    "\n",
    "        remaining_data_str = self._flatten_json(file)\n",
    "        file_and_metadata = FileAndMeatadata(file_string=remaining_data_str, \n",
    "                                             item_id=item_id, \n",
    "                                             main_image_id=main_image_id, \n",
    "                                             other_image_id=other_image_id)\n",
    "        return file_and_metadata\n",
    "    \n",
    "    def _flatten_json(self, y):\n",
    "        \"\"\"Flatten nested JSON into a plain text format.\"\"\"\n",
    "        out = []\n",
    "\n",
    "        def flatten(x):\n",
    "            if isinstance(x, dict):\n",
    "                for v in x.values():\n",
    "                    flatten(v)\n",
    "            elif isinstance(x, list):\n",
    "                for item in x:\n",
    "                    flatten(item)\n",
    "            elif isinstance(x, str):\n",
    "                out.append(x.lower())\n",
    "\n",
    "        flatten(y)\n",
    "        return \" \".join(out)\n",
    "\n",
    "    def _chunk_and_embed(self) -> FAISS:\n",
    "        \"\"\"Chunks file_string, embeds, and creates a FAISS vector store with metadata.\"\"\"\n",
    "        text_splitter = CharacterTextSplitter(chunk_size=self._chunk_size, chunk_overlap=self._chunk_overlap)\n",
    "        chunks_with_metadata = []\n",
    "\n",
    "        # TODO: smarter chunking based on the number of tokens insted of the number of characters.\n",
    "        for item_idx, item in enumerate(self._file_and_metadata_list):\n",
    "            if len(item.file_string) > self._chunk_size:\n",
    "                text_chunks = text_splitter.split_text(item.file_string)\n",
    "                for chunk in text_chunks:\n",
    "                    chunks_with_metadata.append((chunk, {\n",
    "                        \"item_idx\": item_idx,\n",
    "                        \"item_id\": item.item_id,\n",
    "                        \"main_image_id\": item.main_image_id,\n",
    "                        \"other_image_id\": item.other_image_id\n",
    "                    }))\n",
    "            else:\n",
    "                chunks_with_metadata.append((item.file_string, {\n",
    "                    \"item_idx\": item_idx,\n",
    "                        \"item_id\": item.item_id,\n",
    "                        \"main_image_id\": item.main_image_id,\n",
    "                        \"other_image_id\": item.other_image_id\n",
    "                }))\n",
    "\n",
    "        texts = [chunk[0] for chunk in chunks_with_metadata]\n",
    "        metadatas = [chunk[1] for chunk in chunks_with_metadata]\n",
    "\n",
    "        vectorstore = FAISS.from_texts(texts, self._embedding_model, metadatas=metadatas)\n",
    "        return vectorstore"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/var/folders/rq/khqy409s0rs3y1_xhqwh0xn40000gn/T/ipykernel_74688/3381581722.py:17: LangChainDeprecationWarning: The class `HuggingFaceEmbeddings` was deprecated in LangChain 0.2.2 and will be removed in 1.0. An updated version of the class exists in the :class:`~langchain-huggingface package and should be used instead. To use it run `pip install -U :class:`~langchain-huggingface` and import as `from :class:`~langchain_huggingface import HuggingFaceEmbeddings``.\n",
      "  self._embedding_model = HuggingFaceEmbeddings(model_name=embedding_model_name)\n"
     ]
    }
   ],
   "source": [
    "# Example usage:\n",
    "vector_store = VectorStore(file_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'inches centimeters inches centimeters inches centimeters en_gb eono en_gb 【personalized design】full-body and 360 degree protection iphone 11case with translucent matte back and flexible shockproof bumper tpu silicone frame,keep your new iphone 11 secure. compatible with iphone 11 pro-6.1\" (2019) only. en_gb 【fashion translucent】: the back of this iphone 11 case is made up of translucent hard pc, so it can allow the logo of iphone to be seen through it. it also prevents watermark, fingerprints , giving your iphone 11 a clean look. en_gb 【raised edges protect screen and camera lens】edges of this iphone 11 case are a bit raised, giving you extra protection for screen and lens of your iphone 11 and stop the screen and camera lens getting damaged in case if you drop your iphone 11 device on a flat surface. en_gb 【heavy duty protection】 this iphone 11 case has dual layer structure offers iphone 11 maximum full-body rugged protection . made of hard back case and tpu silicone sides and it gives good grip,will stop the iphone 11 from slipping from your hand. en_gb 【wireless charging support】this case completely support wireless charging for your iphone 11 without taking off your iphone 11 case and cable finding troubles, differentiated it from other bulky iphone 11 cases. en_gb black black en_gb eono essential iphone 11(6.1\") case cover, fully protective iphone 11 case with matt hard pc back flexible shockproof bumper tpu silicone phone case for iphone 11-6.1\" (2019) - black\\xa0 pounds kilograms en_gb tpu en_gb silicone en_gb polycarbonate cellular_phone_case en_gb bumper case en_gb coverage en_gb glass en_gb tempered en_gb protector en_gb xr en_gb full gb'"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "query = \"iphone cover\"\n",
    "results = vector_store.perform_search(query)\n",
    "results[0].page_content"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "# class RAGShoppingAssistant:\n",
    "#     def __init__(self, vectorstore):\n",
    "#         self.vectorstore = vectorstore\n",
    "#         self._llm = ChatGoogleGenerativeAI(model=\"gemini-1.5-pro\", api_key=os.getenv('GEMINI_API_KEY'))\n",
    "#         self._memory = ConversationSummaryBufferMemory.from_llm(llm=self._llm, memory_key=\"chat_history\", return_messages=True)\n",
    "\n",
    "#         self._rag_chain = self._setup_rag_chain()\n",
    "\n",
    "#     def _setup_rag_chain(self):\n",
    "#         \"\"\"Sets up the RAG chain with conversation memory.\"\"\"\n",
    "#         template = \"\"\"Use the following pieces of context to answer the question at the end.\n",
    "#         If you don't know the answer, just say you don't know, don't try to make up an answer.\n",
    "#         Also, keep the conversation going, and remember the previous questions and answers.\n",
    "#         {context}\n",
    "#         Question: {question}\n",
    "#         {chat_history}\n",
    "#         Answer:\"\"\"\n",
    "#         prompt = ChatPromptTemplate.from_template(template)\n",
    "#         retriever = self.vectorstore.as_retriever()\n",
    "#         rag_chain = ConversationalRetrievalChain.from_llm(\n",
    "#             self._llm,\n",
    "#             retriever,\n",
    "#             memory=self._memory,\n",
    "#             return_source_documents=True,\n",
    "#         )\n",
    "#         return rag_chain\n",
    "\n",
    "#     def chat_with_assistant(self, question):\n",
    "#         result = self._rag_chain({\"question\": question})\n",
    "#         print(f\"Answer: {result['answer']}\")\n",
    "#         for doc in result['source_documents']:\n",
    "#             print(f\"  - {doc.page_content} (Metadata: {doc.metadata})\")\n",
    "#         return result['question']\n",
    "\n",
    "\n",
    "# assistant = RAGShoppingAssistant(vector_store)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Check out this for RAG\n",
    "# Just review this one https://python.langchain.com/docs/tutorials/rag/\n",
    "# Use this one https://python.langchain.com/docs/tutorials/qa_chat_history/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Basic RAG: search over text, return text and images."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# RAG with text & image embedding."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "shoptalk-chatbot-_PQObPGz-py3.11",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
