{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "import os\n",
    "import chromadb\n",
    "from sentence_transformers import SentenceTransformer\n",
    "from transformers import AutoTokenizer\n",
    "import numpy as np\n",
    "\n",
    "file_path = \"listings/metadata/listings_0.json\"\n",
    "\n",
    "with open(file_path, \"r\", encoding=\"utf-8\") as f:\n",
    "    data = [json.loads(line) for line in f]  # Read each line as a separate JSON object\n",
    "\n",
    "n_data_samples = len(data)\n",
    "print(n_data_samples)  # Number of JSON objects in the file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "all_keys = set()\n",
    "for idx, item in enumerate(data):\n",
    "    for key in item.keys():\n",
    "        if key not in all_keys and idx != 0:\n",
    "            print(f\"idx {idx}, new key is {key}\")\n",
    "    all_keys.update(item.keys())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import json\n",
    "\n",
    "# Directory containing JSON/JSONL files\n",
    "json_folder = \"listings/metadata\"\n",
    "\n",
    "def format_for_rag(data):\n",
    "    \"\"\"General function to format JSON into a structured string for RAG, handling different keys dynamically.\"\"\"\n",
    "    parts = []\n",
    "\n",
    "    def extract_values(value):\n",
    "        \"\"\"Extract values recursively from dicts and lists.\"\"\"\n",
    "        if isinstance(value, dict):\n",
    "            return \", \".join(f\"{k}: {extract_values(v)}\" if k != \"value\" else f\"{extract_values(v)}\" for k, v in value.items())\n",
    "        elif isinstance(value, list):\n",
    "            return \", \".join(str(extract_values(v)) for v in value)\n",
    "        return str(value)\n",
    "\n",
    "    for key, value in data.items():\n",
    "        formatted_value = extract_values(value)\n",
    "        parts.append(f\"{key.capitalize()}: {formatted_value}\")\n",
    "\n",
    "    return \"\\n\".join(parts)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "formatted_data_sample = format_for_rag(data[1342]) \n",
    "# Some strings contain too many characters.\n",
    "# There may be smarter ways to generate the formatted string...\n",
    "# Also, so descriptions are not in English, one idea my be to translate them to English.\n",
    "\n",
    "print(formatted_data_sample)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ChromaDB with chuncking https://chatgpt.com/share/67c66fd2-3b00-800f-9b27-ad2a00ec8ae6"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "formatted_data = np.array([format_for_rag(data[i]) for i in range(n_data_samples)])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize ChromaDB client\n",
    "chroma_client = chromadb.PersistentClient(path=\"./chroma_db\")\n",
    "collection = chroma_client.get_or_create_collection(name=\"products\")\n",
    "\n",
    "# Load embedding model (Sentence Transformers or OpenAI)\n",
    "embedding_model = SentenceTransformer(\"all-MiniLM-L6-v2\") \n",
    "tokenizer = AutoTokenizer.from_pretrained(\"sentence-transformers/all-MiniLM-L6-v2\")\n",
    "# By default, input text longer than 256 word pieces is truncated."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 128 tokens because of the way the model was trained.\n",
    "def chunk_text(text, max_tokens=128):\n",
    "    \"\"\"Split text into chunks of max_tokens using a tokenizer.\"\"\"\n",
    "    tokens = tokenizer.encode(text, add_special_tokens=False)\n",
    "    chunks = [tokens[i:i+max_tokens] for i in range(0, len(tokens), max_tokens)]\n",
    "    return [tokenizer.decode(chunk) for chunk in chunks]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Process each text entry\n",
    "n_samples = 100\n",
    "\n",
    "for i, text in enumerate(formatted_data[:n_samples]):\n",
    "    text_chunks = chunk_text(text)  # Split text into chunks\n",
    "\n",
    "    # Generate embeddings for each chunk\n",
    "    embeddings = embedding_model.encode(text_chunks)\n",
    "\n",
    "    # Store in ChromaDB\n",
    "    for j, (chunk, embedding) in enumerate(zip(text_chunks, embeddings)):\n",
    "        collection.add(\n",
    "            ids=[f\"item_{i}_chunk_{j}\"],  # Unique ID per chunk\n",
    "            embeddings=[embedding.tolist()],  # Convert NumPy array to list\n",
    "            metadatas=[{\"original_id\": i, \"chunk_index\": j, \"text\": chunk}]\n",
    "        )\n",
    "\n",
    "    print(f\"‚úÖ Processed {len(text_chunks)} chunks for text entry {i}\")\n",
    "\n",
    "print(\"üöÄ All data stored in ChromaDB successfully!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def query_chroma(query_text, collection, top_k=3):\n",
    "    \"\"\"Retrieve the most relevant text chunks from ChromaDB based on query.\"\"\"\n",
    "    \n",
    "    # Generate embedding for the query\n",
    "    query_embedding = embedding_model.encode([query_text])\n",
    "\n",
    "    # Perform similarity search in ChromaDB\n",
    "    results = collection.query(\n",
    "        query_embeddings=query_embedding.tolist(),\n",
    "        n_results=top_k  # Retrieve top K most relevant results\n",
    "    )\n",
    "\n",
    "    # Display retrieved results\n",
    "    print(f\"\\nüîç Query: {query_text}\\n\")\n",
    "    for i, match in enumerate(results[\"metadatas\"][0]):  # First query result batch\n",
    "        print(f\"‚ú® Match {i+1}:\")\n",
    "        print(f\"üîπ Original ID: {match['original_id']}\")\n",
    "        print(f\"üîπ Chunk Index: {match['chunk_index']}\")\n",
    "        print(f\"üîπ Text: {match['text']}\\n\")\n",
    "        print(\"=\" * 80)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Example Query\n",
    "query_text = \"I love cats and I want a mobile phone cover with multiple colors for samsung\"\n",
    "query_chroma(query_text, collection, top_k=5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(formatted_data[147])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "shoptalk-chatbot-_PQObPGz-py3.11",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
